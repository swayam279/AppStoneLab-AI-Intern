{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5383c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, transforms, Normalize\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms.functional import pad\n",
    "import torchvision\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767004fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 MB\n",
      "Cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(device) / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f591b",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824d4125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6375, 0.7364, 0.7428])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(3)\n",
    "print(x)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa606047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "552b9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D tensor: tensor([1, 2, 3, 4, 5])\n",
      "\n",
      "2D Tensor: tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "3D Tensor: tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "t1=torch.tensor([1,2,3,4,5])\n",
    "print('1D tensor:',t1)\n",
    "\n",
    "t2=torch.tensor([[1,2],[3,4],[5,6]])\n",
    "print('\\n2D Tensor:',t2)\n",
    "\n",
    "t3=torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "print('\\n3D Tensor:',t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "25ed39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 2x3 tensor:\n",
      " tensor([[0.0929, 0.8024, 0.5947],\n",
      "        [0.1535, 0.9415, 0.4627]])\n",
      "\n",
      "2x3 tensor filled with zeros:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "2x3 tensor filled with ones:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "2x3 empty tensor:\n",
      " tensor([[ 1.1364e-03, -4.8785e-05,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "t4=torch.rand(2,3)\n",
    "print('random 2x3 tensor:\\n',t4)\n",
    "\n",
    "t5=torch.zeros(2,3)\n",
    "print('\\n2x3 tensor filled with zeros:\\n',t5)\n",
    "\n",
    "t6=torch.ones(2,3)\n",
    "print('\\n2x3 tensor filled with ones:\\n',t6)\n",
    "\n",
    "t7=torch.empty(2,3)\n",
    "print('\\n2x3 empty tensor:\\n',t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5c761326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "First row: tensor([1, 2])\n",
      "\n",
      "First column: tensor([1, 3, 5])\n",
      "\n",
      "Last column: tensor([2, 4, 6])\n",
      "\n",
      "First 2 rows:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "t8=torch.tensor([[1,2],[3,4],[5,6]])\n",
    "print('2D tensor:\\n',t8)\n",
    "\n",
    "print('\\nFirst row:',t8[0])\n",
    "\n",
    "print('\\nFirst column:',t8[:,0])\n",
    "\n",
    "print('\\nLast column:',t8[:,-1])\n",
    "\n",
    "print('\\nFirst 2 rows:\\n',t8[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49cf2efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2D tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "Reshaped 2D tensor:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "t9=torch.tensor([[1,2],[3,4],[5,6]])\n",
    "t9_reshape=t9.view(2,3)\n",
    "print('Original 2D tensor:\\n',t9)\n",
    "\n",
    "print('\\nReshaped 2D tensor:\\n',t9_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40cc0137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2D tensor:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Added tensor:\n",
      " tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n",
      "\n",
      "Matmul tensor:\n",
      " tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "t10=torch.tensor([[1,2,3],[4,5,6]])\n",
    "t11=torch.tensor([[10,20,30]])\n",
    "add=t10+t11\n",
    "print('Original 2D tensor:\\n',t10)\n",
    "\n",
    "print('\\nAdded tensor:\\n',add)\n",
    "\n",
    "matmul=torch.matmul(t10, t10.T)\n",
    "print('\\nMatmul tensor:\\n',matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "48b98aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Current GPU memory usage:\n",
      "Allocated: 0.12 MB\n",
      "Cached: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "tensor_size = (100, 100)  \n",
    "a = torch.randn(tensor_size, device=device)  \n",
    "b = torch.randn(tensor_size, device=device)  \n",
    "\n",
    "c = a + b  \n",
    "\n",
    "\n",
    "print(\"Current GPU memory usage:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(device) / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db4308",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a14deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data=datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, #extracts test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size=60\n",
    "train_dataloader=DataLoader(training_data, batch_size)\n",
    "test_dataloader=DataLoader(test_data, batch_size)\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "classes = list(labels_map.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf99622",
   "metadata": {},
   "source": [
    "## Fully Connected Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47587270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1=nn.Linear(2,4)\n",
    "        self.fc2=nn.Linear(4,1)\n",
    "    def forward(self,x):\n",
    "        x=torch.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "model=SimpleNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3bc70488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.3065\n",
      "Epoch [200/1000], Loss: 0.2618\n",
      "Epoch [300/1000], Loss: 0.2404\n",
      "Epoch [400/1000], Loss: 0.2232\n",
      "Epoch [500/1000], Loss: 0.2077\n",
      "Epoch [600/1000], Loss: 0.1932\n",
      "Epoch [700/1000], Loss: 0.1803\n",
      "Epoch [800/1000], Loss: 0.1678\n",
      "Epoch [900/1000], Loss: 0.1556\n",
      "Epoch [1000/1000], Loss: 0.1436\n"
     ]
    }
   ],
   "source": [
    "X_train=torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y_train=torch.tensor([[0.0],[1.0],[1.0],[0.0]])\n",
    "\n",
    "model=SimpleNN()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    outputs=model(X_train)\n",
    "    loss=criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "afa32de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3884],\n",
      "        [0.6384],\n",
      "        [0.5996],\n",
      "        [0.3633]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data=torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "    predictions=model(test_data)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5190074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "        nn.Linear(28*28,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x=self.flatten(x)\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model=NeuralNetwork()\n",
    "print(model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf93b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch % 100 == 0:\n",
    "    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "    print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size=len(dataloader.dataset)    \n",
    "    n_batches=len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct= 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred, y).item()\n",
    "            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss/=n_batches\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2973045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n",
      "loss: 1.5082645416259766  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.473011 \n",
      "\n",
      "Epoch 2:\n",
      "\n",
      "loss: 1.0015138387680054  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.959777 \n",
      "\n",
      "Epoch 3:\n",
      "\n",
      "loss: 0.846994161605835  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.803486 \n",
      "\n",
      "Epoch 4:\n",
      "\n",
      "loss: 0.7590461373329163  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.725632 \n",
      "\n",
      "Epoch 5:\n",
      "\n",
      "loss: 0.6968614459037781  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.672210 \n",
      "\n",
      "Epoch 6:\n",
      "\n",
      "loss: 0.6495867371559143  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.631773 \n",
      "\n",
      "Epoch 7:\n",
      "\n",
      "loss: 0.6136932373046875  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.600784 \n",
      "\n",
      "Epoch 8:\n",
      "\n",
      "loss: 0.5865381360054016  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.576731 \n",
      "\n",
      "Epoch 9:\n",
      "\n",
      "loss: 0.5661325454711914  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.557673 \n",
      "\n",
      "Epoch 10:\n",
      "\n",
      "loss: 0.5501990914344788  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.542264 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}:\\n\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227b72fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle Boot\", Actual: \"Ankle Boot\"\n",
      "Predicted: \"Pullover\", Actual: \"Pullover\"\n",
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
      "Predicted: \"Shirt\", Actual: \"Shirt\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        X, y = test_data[i][0], test_data[i][1]\n",
    "        pred = model(X)\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974e7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight -- torch.Size([512, 784])\n",
      "linear_relu_stack.0.bias -- torch.Size([512])\n",
      "linear_relu_stack.2.weight -- torch.Size([512, 512])\n",
      "linear_relu_stack.2.bias -- torch.Size([512])\n",
      "linear_relu_stack.4.weight -- torch.Size([10, 512])\n",
      "linear_relu_stack.4.bias -- torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name, \"--\",parameter.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 50])\n",
      "torch.Size([20, 33, 24])\n",
      "Conv1d(16, 33, kernel_size=(3,), stride=(2,))\n"
     ]
    }
   ],
   "source": [
    "# Convolution 1D:\n",
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "print(input.size())\n",
    "print(output.size())\n",
    "# print(output)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d764684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 50, 100])\n",
      "torch.Size([20, 33, 24, 49])\n",
      "torch.Size([20, 33, 28, 100])\n",
      "torch.Size([20, 33, 26, 100])\n"
     ]
    }
   ],
   "source": [
    "# Convolution 2D\n",
    "\n",
    "m1 = nn.Conv2d(16, 33, 3, stride=2)\n",
    "\n",
    "m2 = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "\n",
    "m3 = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output1 = m1(input)\n",
    "output2 = m2(input)\n",
    "output3 = m3(input)\n",
    "\n",
    "print(input.size())\n",
    "print(output1.size())\n",
    "print(output2.size())\n",
    "print(output3.size())\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0308826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 10, 50, 50])\n",
      "torch.Size([20, 33, 5, 25, 25])\n",
      "torch.Size([20, 33, 8, 50, 49])\n"
     ]
    }
   ],
   "source": [
    "# Convolution 3D\n",
    "\n",
    "m1=nn.Conv3d(16, 33, 2, stride=2)\n",
    "m2=nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
    "input=torch.randn(20, 16, 10, 50, 50)\n",
    "output1=m1(input)\n",
    "output2=m2(input)\n",
    "print(input.size())\n",
    "print(output1.size())\n",
    "print(output2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a989a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#dropout \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "        nn.Linear(28*28,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x=self.flatten(x)\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model=NeuralNetwork()\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b78c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n",
      "loss: 0.43239572644233704  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.451494 \n",
      "\n",
      "Epoch 2:\n",
      "\n",
      "loss: 0.38935422897338867  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.399360 \n",
      "\n",
      "Epoch 3:\n",
      "\n",
      "loss: 0.3372545540332794  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.378803 \n",
      "\n",
      "Epoch 4:\n",
      "\n",
      "loss: 0.3212348222732544  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.354543 \n",
      "\n",
      "Epoch 5:\n",
      "\n",
      "loss: 0.36041709780693054  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.342899 \n",
      "\n",
      "Epoch 6:\n",
      "\n",
      "loss: 0.2732643187046051  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.352029 \n",
      "\n",
      "Epoch 7:\n",
      "\n",
      "loss: 0.27149611711502075  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.341071 \n",
      "\n",
      "Epoch 8:\n",
      "\n",
      "loss: 0.3338007628917694  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.334360 \n",
      "\n",
      "Epoch 9:\n",
      "\n",
      "loss: 0.22312143445014954  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.341208 \n",
      "\n",
      "Epoch 10:\n",
      "\n",
      "loss: 0.26701176166534424  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.333266 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "def training(dataLoader, model, loss_fn, optimizer):\n",
    "    size=len(dataLoader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch % 100 == 0:\n",
    "    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "    print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "            \n",
    "def testing(dataLoader, model, loss_fn):\n",
    "    size=len(dataLoader.dataset)    \n",
    "    n_batches=len(dataLoader)\n",
    "    model.eval()\n",
    "    test_loss, correct= 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred, y).item()\n",
    "            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss/=n_batches\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "epochs=10\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}:\\n\")\n",
    "    training(train_dataloader, model, loss_fn, optimizer)\n",
    "    testing(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b97f4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
      "Predicted: \"Pullover\", Actual: \"Pullover\"\n",
      "Predicted: \"Pullover\", Actual: \"Coat\"\n",
      "Predicted: \"Bag\", Actual: \"Bag\"\n",
      "Predicted: \"T-Shirt\", Actual: \"T-Shirt\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(15,20):\n",
    "        X, y = test_data[i][0], test_data[i][1]\n",
    "        pred = model(X)\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88371cad",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7a24789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Linear(in_features=4608, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1= nn.Conv2d(1, 64, 2)\n",
    "        self.conv2= nn.Conv2d(64, 128, 2)\n",
    "        self.relu= nn.ReLU()\n",
    "        self.pool= nn.MaxPool2d(2)\n",
    "        self.fc_shape=None\n",
    "        self.flat= nn.Flatten()\n",
    "        self.fc_shape= self.size_tracker(torch.randn(1, 1, 28, 28))\n",
    "        self.fc= nn.Linear(self.fc_shape, 10)\n",
    "        \n",
    "    def size_tracker(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.flat(x)\n",
    "        return x.shape[1]   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.pool(self.relu(self.conv1(x)))\n",
    "        x= self.pool(self.relu(self.conv2(x)))\n",
    "        x= self.flat(x)\n",
    "        x= self.fc(x)\n",
    "        return x\n",
    "\n",
    "model=CNN()\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0822d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "def training(dataLoader, model, loss_fn, optimizer):\n",
    "    size=len(dataLoader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch % 100 == 0:\n",
    "    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "    print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "\n",
    "def testing(dataLoader, model, loss_fn):\n",
    "    size=len(dataLoader.dataset)    \n",
    "    n_batches=len(dataLoader)\n",
    "    model.eval()\n",
    "    test_loss, correct= 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred, y).item()\n",
    "            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss/=n_batches\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ac0f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n",
      "loss: 0.5040072798728943  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.525819 \n",
      "\n",
      "Epoch 2:\n",
      "\n",
      "loss: 0.41093847155570984  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.449781 \n",
      "\n",
      "Epoch 3:\n",
      "\n",
      "loss: 0.3659093976020813  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.415581 \n",
      "\n",
      "Epoch 4:\n",
      "\n",
      "loss: 0.33310309052467346  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.393739 \n",
      "\n",
      "Epoch 5:\n",
      "\n",
      "loss: 0.3093981444835663  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.377560 \n",
      "\n",
      "Epoch 6:\n",
      "\n",
      "loss: 0.2918350398540497  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.365336 \n",
      "\n",
      "Epoch 7:\n",
      "\n",
      "loss: 0.27667614817619324  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.355653 \n",
      "\n",
      "Epoch 8:\n",
      "\n",
      "loss: 0.26431235671043396  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.347877 \n",
      "\n",
      "Epoch 9:\n",
      "\n",
      "loss: 0.2540619969367981  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.341319 \n",
      "\n",
      "Epoch 10:\n",
      "\n",
      "loss: 0.2446449249982834  [60000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.335718 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=10\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch {i+1}:\\n\")\n",
    "    training(train_dataloader, model, loss_fn, optimizer)\n",
    "    testing(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f44088dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Pullover\", Actual: \"Pullover\"\n",
      "Predicted: \"Sneaker\", Actual: \"Sandal\"\n",
      "Predicted: \"Sneaker\", Actual: \"Sneaker\"\n",
      "Predicted: \"Sandal\", Actual: \"Ankle Boot\"\n",
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(20,25):\n",
    "        X, y = test_data[i][0], test_data[i][1]\n",
    "        X = X.unsqueeze(0)\n",
    "        pred = model(X)\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8092d",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b121a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet50_Weights.IMAGENET1K_V2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_dir= Path(\"D:\\\\AppStoneLab\\\\Pytorch\\\\data\\\\pizza_steak_sushi\\\\train\")\n",
    "test_dir= Path(\"D:\\\\AppStoneLab\\\\Pytorch\\\\data\\\\pizza_steak_sushi\\\\test\")\n",
    "\n",
    "weights= torchvision.models.ResNet50_Weights.DEFAULT\n",
    "\n",
    "auto_transforms= weights.transforms() #automaticalaly fetches transformations used for resnet dataset\n",
    "auto_transforms\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683f9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(train_dir, transform=auto_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=auto_transforms)\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader( test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# train_dataloader, test_dataloader, class_names\n",
    "# for X,y in train_dataloader:\n",
    "#     print(X.shape, y.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3ad779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=models.resnet50(weights=weights).to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4f164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                    [-1, 3]           6,147\n",
      "================================================================\n",
      "Total params: 23,514,179\n",
      "Trainable params: 6,147\n",
      "Non-trainable params: 23,508,032\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.55\n",
      "Params size (MB): 89.70\n",
      "Estimated Total Size (MB): 376.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad=True\n",
    "\n",
    "model.fc=nn.Linear(2048, 3)\n",
    "model.to(device)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fd7dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn= nn.CrossEntropyLoss()\n",
    "optimizer=optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "def training(dataLoader, model, loss_fn, optimizer):\n",
    "    size=len(dataLoader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "    print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "    \n",
    "def testing(dataLoader, model, loss_fn):\n",
    "    size=len(dataLoader.dataset)    \n",
    "    n_batches=len(dataLoader)\n",
    "    model.eval()\n",
    "    test_loss, correct= 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred, y).item()\n",
    "\n",
    "            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss/=n_batches\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3a4aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n",
      "loss: 1.0324268341064453  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.787780 \n",
      "\n",
      "Epoch 2:\n",
      "\n",
      "loss: 1.15571928024292  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.703075 \n",
      "\n",
      "Epoch 3:\n",
      "\n",
      "loss: 1.2716748714447021  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.722085 \n",
      "\n",
      "Epoch 4:\n",
      "\n",
      "loss: 0.9466521739959717  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.675305 \n",
      "\n",
      "Epoch 5:\n",
      "\n",
      "loss: 0.8595020771026611  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.647986 \n",
      "\n",
      "Epoch 6:\n",
      "\n",
      "loss: 0.9800182580947876  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.623761 \n",
      "\n",
      "Epoch 7:\n",
      "\n",
      "loss: 1.0992772579193115  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.603905 \n",
      "\n",
      "Epoch 8:\n",
      "\n",
      "loss: 0.7749406695365906  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.510256 \n",
      "\n",
      "Epoch 9:\n",
      "\n",
      "loss: 1.13480806350708  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.560505 \n",
      "\n",
      "Epoch 10:\n",
      "\n",
      "loss: 0.9455206394195557  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.509020 \n",
      "\n",
      "Epoch 11:\n",
      "\n",
      "loss: 1.2691264152526855  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.512145 \n",
      "\n",
      "Epoch 12:\n",
      "\n",
      "loss: 0.8030563592910767  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.524442 \n",
      "\n",
      "Epoch 13:\n",
      "\n",
      "loss: 1.1872203350067139  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.526300 \n",
      "\n",
      "Epoch 14:\n",
      "\n",
      "loss: 1.3554916381835938  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.488747 \n",
      "\n",
      "Epoch 15:\n",
      "\n",
      "loss: 1.2550253868103027  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.473541 \n",
      "\n",
      "Epoch 16:\n",
      "\n",
      "loss: 1.0899157524108887  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.486900 \n",
      "\n",
      "Epoch 17:\n",
      "\n",
      "loss: 0.9484817981719971  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.532372 \n",
      "\n",
      "Epoch 18:\n",
      "\n",
      "loss: 1.1831321716308594  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.560906 \n",
      "\n",
      "Epoch 19:\n",
      "\n",
      "loss: 1.2301019430160522  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.514748 \n",
      "\n",
      "Epoch 20:\n",
      "\n",
      "loss: 1.5059211254119873  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.481695 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=20\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch {i+1}:\\n\")\n",
    "    training(train_dataloader, model, loss_fn, optimizer)\n",
    "    testing(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39db8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 12, Predicted: \"steak\", Actual: \"pizza\"\n",
      "at 29, Predicted: \"sushi\", Actual: \"steak\"\n",
      "at 51, Predicted: \"pizza\", Actual: \"sushi\"\n",
      "at 55, Predicted: \"pizza\", Actual: \"sushi\"\n",
      "at 63, Predicted: \"pizza\", Actual: \"sushi\"\n",
      "at 70, Predicted: \"steak\", Actual: \"sushi\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_data)):\n",
    "        X, y = test_data[i][0], test_data[i][1]\n",
    "        X = X.unsqueeze(0)\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        predicted, actual = class_names[pred[0].argmax(0)], class_names[y]\n",
    "        if predicted != actual:\n",
    "            print(f'at {i}, Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e20e7",
   "metadata": {},
   "source": [
    "Partition -------------------- EfficientNet_B0-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights= torchvision.models.EfficientNet_B0_Weights.DEFAULT \n",
    "\n",
    "auto_transforms= weights.transforms() #Automaticalaly fetches transformations used for EfficientNet dataset\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98254d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(train_dir, transform=auto_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=auto_transforms)\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader( test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0789079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "              SiLU-6         [-1, 32, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             264\n",
      "              SiLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-11             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 32, 112, 112]               0\n",
      "           Conv2d-13         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-14         [-1, 16, 112, 112]              32\n",
      "           MBConv-15         [-1, 16, 112, 112]               0\n",
      "           Conv2d-16         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-17         [-1, 96, 112, 112]             192\n",
      "             SiLU-18         [-1, 96, 112, 112]               0\n",
      "           Conv2d-19           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-20           [-1, 96, 56, 56]             192\n",
      "             SiLU-21           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0\n",
      "           Conv2d-23              [-1, 4, 1, 1]             388\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-26             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-27           [-1, 96, 56, 56]               0\n",
      "           Conv2d-28           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-29           [-1, 24, 56, 56]              48\n",
      "           MBConv-30           [-1, 24, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-32          [-1, 144, 56, 56]             288\n",
      "             SiLU-33          [-1, 144, 56, 56]               0\n",
      "           Conv2d-34          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-35          [-1, 144, 56, 56]             288\n",
      "             SiLU-36          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0\n",
      "           Conv2d-38              [-1, 6, 1, 1]             870\n",
      "             SiLU-39              [-1, 6, 1, 1]               0\n",
      "           Conv2d-40            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-41            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-42          [-1, 144, 56, 56]               0\n",
      "           Conv2d-43           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-44           [-1, 24, 56, 56]              48\n",
      "  StochasticDepth-45           [-1, 24, 56, 56]               0\n",
      "           MBConv-46           [-1, 24, 56, 56]               0\n",
      "           Conv2d-47          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-48          [-1, 144, 56, 56]             288\n",
      "             SiLU-49          [-1, 144, 56, 56]               0\n",
      "           Conv2d-50          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-51          [-1, 144, 28, 28]             288\n",
      "             SiLU-52          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n",
      "           Conv2d-54              [-1, 6, 1, 1]             870\n",
      "             SiLU-55              [-1, 6, 1, 1]               0\n",
      "           Conv2d-56            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-57            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-58          [-1, 144, 28, 28]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "           MBConv-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-63          [-1, 240, 28, 28]             480\n",
      "             SiLU-64          [-1, 240, 28, 28]               0\n",
      "           Conv2d-65          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-66          [-1, 240, 28, 28]             480\n",
      "             SiLU-67          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0\n",
      "           Conv2d-69             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-70             [-1, 10, 1, 1]               0\n",
      "           Conv2d-71            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-72            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-73          [-1, 240, 28, 28]               0\n",
      "           Conv2d-74           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-75           [-1, 40, 28, 28]              80\n",
      "  StochasticDepth-76           [-1, 40, 28, 28]               0\n",
      "           MBConv-77           [-1, 40, 28, 28]               0\n",
      "           Conv2d-78          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-79          [-1, 240, 28, 28]             480\n",
      "             SiLU-80          [-1, 240, 28, 28]               0\n",
      "           Conv2d-81          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-82          [-1, 240, 14, 14]             480\n",
      "             SiLU-83          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0\n",
      "           Conv2d-85             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-86             [-1, 10, 1, 1]               0\n",
      "           Conv2d-87            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-88            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-89          [-1, 240, 14, 14]               0\n",
      "           Conv2d-90           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-91           [-1, 80, 14, 14]             160\n",
      "           MBConv-92           [-1, 80, 14, 14]               0\n",
      "           Conv2d-93          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-94          [-1, 480, 14, 14]             960\n",
      "             SiLU-95          [-1, 480, 14, 14]               0\n",
      "           Conv2d-96          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-97          [-1, 480, 14, 14]             960\n",
      "             SiLU-98          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0\n",
      "          Conv2d-100             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-101             [-1, 20, 1, 1]               0\n",
      "          Conv2d-102            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-103            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-104          [-1, 480, 14, 14]               0\n",
      "          Conv2d-105           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-106           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-107           [-1, 80, 14, 14]               0\n",
      "          MBConv-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      "            SiLU-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "            SiLU-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Conv2d-116             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-117             [-1, 20, 1, 1]               0\n",
      "          Conv2d-118            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-119            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-123           [-1, 80, 14, 14]               0\n",
      "          MBConv-124           [-1, 80, 14, 14]               0\n",
      "          Conv2d-125          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-126          [-1, 480, 14, 14]             960\n",
      "            SiLU-127          [-1, 480, 14, 14]               0\n",
      "          Conv2d-128          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-129          [-1, 480, 14, 14]             960\n",
      "            SiLU-130          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0\n",
      "          Conv2d-132             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-133             [-1, 20, 1, 1]               0\n",
      "          Conv2d-134            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-135            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-136          [-1, 480, 14, 14]               0\n",
      "          Conv2d-137          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-138          [-1, 112, 14, 14]             224\n",
      "          MBConv-139          [-1, 112, 14, 14]               0\n",
      "          Conv2d-140          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-142          [-1, 672, 14, 14]               0\n",
      "          Conv2d-143          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-144          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-145          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0\n",
      "          Conv2d-147             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-148             [-1, 28, 1, 1]               0\n",
      "          Conv2d-149            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-150            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-151          [-1, 672, 14, 14]               0\n",
      "          Conv2d-152          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-153          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-154          [-1, 112, 14, 14]               0\n",
      "          MBConv-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-157          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-158          [-1, 672, 14, 14]               0\n",
      "          Conv2d-159          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-161          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n",
      "          Conv2d-163             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-164             [-1, 28, 1, 1]               0\n",
      "          Conv2d-165            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-166            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-167          [-1, 672, 14, 14]               0\n",
      "          Conv2d-168          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-170          [-1, 112, 14, 14]               0\n",
      "          MBConv-171          [-1, 112, 14, 14]               0\n",
      "          Conv2d-172          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-174          [-1, 672, 14, 14]               0\n",
      "          Conv2d-175            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-176            [-1, 672, 7, 7]           1,344\n",
      "            SiLU-177            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0\n",
      "          Conv2d-179             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-180             [-1, 28, 1, 1]               0\n",
      "          Conv2d-181            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-182            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-183            [-1, 672, 7, 7]               0\n",
      "          Conv2d-184            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-185            [-1, 192, 7, 7]             384\n",
      "          MBConv-186            [-1, 192, 7, 7]               0\n",
      "          Conv2d-187           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-189           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-190           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-191           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-192           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-194             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-195             [-1, 48, 1, 1]               0\n",
      "          Conv2d-196           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-197           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-198           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-199            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-200            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-201            [-1, 192, 7, 7]               0\n",
      "          MBConv-202            [-1, 192, 7, 7]               0\n",
      "          Conv2d-203           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-204           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-205           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-206           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-208           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-210             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-211             [-1, 48, 1, 1]               0\n",
      "          Conv2d-212           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-213           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-214           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-215            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-216            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-217            [-1, 192, 7, 7]               0\n",
      "          MBConv-218            [-1, 192, 7, 7]               0\n",
      "          Conv2d-219           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-220           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-221           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-222           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-224           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-226             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-227             [-1, 48, 1, 1]               0\n",
      "          Conv2d-228           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-229           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-230           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-231            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-232            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-233            [-1, 192, 7, 7]               0\n",
      "          MBConv-234            [-1, 192, 7, 7]               0\n",
      "          Conv2d-235           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-236           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-237           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-238           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-240           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-242             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-243             [-1, 48, 1, 1]               0\n",
      "          Conv2d-244           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-245           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-246           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-247            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-248            [-1, 320, 7, 7]             640\n",
      "          MBConv-249            [-1, 320, 7, 7]               0\n",
      "          Conv2d-250           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-251           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-252           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0\n",
      "         Dropout-254                 [-1, 1280]               0\n",
      "          Linear-255                 [-1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 5,288,548\n",
      "Trainable params: 5,288,548\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 173.65\n",
      "Params size (MB): 20.17\n",
      "Estimated Total Size (MB): 194.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model= models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "119e3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "              SiLU-6         [-1, 32, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             264\n",
      "              SiLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-11             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 32, 112, 112]               0\n",
      "           Conv2d-13         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-14         [-1, 16, 112, 112]              32\n",
      "           MBConv-15         [-1, 16, 112, 112]               0\n",
      "           Conv2d-16         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-17         [-1, 96, 112, 112]             192\n",
      "             SiLU-18         [-1, 96, 112, 112]               0\n",
      "           Conv2d-19           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-20           [-1, 96, 56, 56]             192\n",
      "             SiLU-21           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0\n",
      "           Conv2d-23              [-1, 4, 1, 1]             388\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-26             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-27           [-1, 96, 56, 56]               0\n",
      "           Conv2d-28           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-29           [-1, 24, 56, 56]              48\n",
      "           MBConv-30           [-1, 24, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-32          [-1, 144, 56, 56]             288\n",
      "             SiLU-33          [-1, 144, 56, 56]               0\n",
      "           Conv2d-34          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-35          [-1, 144, 56, 56]             288\n",
      "             SiLU-36          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0\n",
      "           Conv2d-38              [-1, 6, 1, 1]             870\n",
      "             SiLU-39              [-1, 6, 1, 1]               0\n",
      "           Conv2d-40            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-41            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-42          [-1, 144, 56, 56]               0\n",
      "           Conv2d-43           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-44           [-1, 24, 56, 56]              48\n",
      "  StochasticDepth-45           [-1, 24, 56, 56]               0\n",
      "           MBConv-46           [-1, 24, 56, 56]               0\n",
      "           Conv2d-47          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-48          [-1, 144, 56, 56]             288\n",
      "             SiLU-49          [-1, 144, 56, 56]               0\n",
      "           Conv2d-50          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-51          [-1, 144, 28, 28]             288\n",
      "             SiLU-52          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n",
      "           Conv2d-54              [-1, 6, 1, 1]             870\n",
      "             SiLU-55              [-1, 6, 1, 1]               0\n",
      "           Conv2d-56            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-57            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-58          [-1, 144, 28, 28]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "           MBConv-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-63          [-1, 240, 28, 28]             480\n",
      "             SiLU-64          [-1, 240, 28, 28]               0\n",
      "           Conv2d-65          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-66          [-1, 240, 28, 28]             480\n",
      "             SiLU-67          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0\n",
      "           Conv2d-69             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-70             [-1, 10, 1, 1]               0\n",
      "           Conv2d-71            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-72            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-73          [-1, 240, 28, 28]               0\n",
      "           Conv2d-74           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-75           [-1, 40, 28, 28]              80\n",
      "  StochasticDepth-76           [-1, 40, 28, 28]               0\n",
      "           MBConv-77           [-1, 40, 28, 28]               0\n",
      "           Conv2d-78          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-79          [-1, 240, 28, 28]             480\n",
      "             SiLU-80          [-1, 240, 28, 28]               0\n",
      "           Conv2d-81          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-82          [-1, 240, 14, 14]             480\n",
      "             SiLU-83          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0\n",
      "           Conv2d-85             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-86             [-1, 10, 1, 1]               0\n",
      "           Conv2d-87            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-88            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-89          [-1, 240, 14, 14]               0\n",
      "           Conv2d-90           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-91           [-1, 80, 14, 14]             160\n",
      "           MBConv-92           [-1, 80, 14, 14]               0\n",
      "           Conv2d-93          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-94          [-1, 480, 14, 14]             960\n",
      "             SiLU-95          [-1, 480, 14, 14]               0\n",
      "           Conv2d-96          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-97          [-1, 480, 14, 14]             960\n",
      "             SiLU-98          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0\n",
      "          Conv2d-100             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-101             [-1, 20, 1, 1]               0\n",
      "          Conv2d-102            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-103            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-104          [-1, 480, 14, 14]               0\n",
      "          Conv2d-105           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-106           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-107           [-1, 80, 14, 14]               0\n",
      "          MBConv-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      "            SiLU-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "            SiLU-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Conv2d-116             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-117             [-1, 20, 1, 1]               0\n",
      "          Conv2d-118            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-119            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-123           [-1, 80, 14, 14]               0\n",
      "          MBConv-124           [-1, 80, 14, 14]               0\n",
      "          Conv2d-125          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-126          [-1, 480, 14, 14]             960\n",
      "            SiLU-127          [-1, 480, 14, 14]               0\n",
      "          Conv2d-128          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-129          [-1, 480, 14, 14]             960\n",
      "            SiLU-130          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0\n",
      "          Conv2d-132             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-133             [-1, 20, 1, 1]               0\n",
      "          Conv2d-134            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-135            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-136          [-1, 480, 14, 14]               0\n",
      "          Conv2d-137          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-138          [-1, 112, 14, 14]             224\n",
      "          MBConv-139          [-1, 112, 14, 14]               0\n",
      "          Conv2d-140          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-142          [-1, 672, 14, 14]               0\n",
      "          Conv2d-143          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-144          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-145          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0\n",
      "          Conv2d-147             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-148             [-1, 28, 1, 1]               0\n",
      "          Conv2d-149            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-150            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-151          [-1, 672, 14, 14]               0\n",
      "          Conv2d-152          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-153          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-154          [-1, 112, 14, 14]               0\n",
      "          MBConv-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-157          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-158          [-1, 672, 14, 14]               0\n",
      "          Conv2d-159          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-161          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n",
      "          Conv2d-163             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-164             [-1, 28, 1, 1]               0\n",
      "          Conv2d-165            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-166            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-167          [-1, 672, 14, 14]               0\n",
      "          Conv2d-168          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-170          [-1, 112, 14, 14]               0\n",
      "          MBConv-171          [-1, 112, 14, 14]               0\n",
      "          Conv2d-172          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-174          [-1, 672, 14, 14]               0\n",
      "          Conv2d-175            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-176            [-1, 672, 7, 7]           1,344\n",
      "            SiLU-177            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0\n",
      "          Conv2d-179             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-180             [-1, 28, 1, 1]               0\n",
      "          Conv2d-181            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-182            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-183            [-1, 672, 7, 7]               0\n",
      "          Conv2d-184            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-185            [-1, 192, 7, 7]             384\n",
      "          MBConv-186            [-1, 192, 7, 7]               0\n",
      "          Conv2d-187           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-189           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-190           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-191           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-192           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-194             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-195             [-1, 48, 1, 1]               0\n",
      "          Conv2d-196           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-197           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-198           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-199            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-200            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-201            [-1, 192, 7, 7]               0\n",
      "          MBConv-202            [-1, 192, 7, 7]               0\n",
      "          Conv2d-203           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-204           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-205           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-206           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-208           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-210             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-211             [-1, 48, 1, 1]               0\n",
      "          Conv2d-212           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-213           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-214           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-215            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-216            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-217            [-1, 192, 7, 7]               0\n",
      "          MBConv-218            [-1, 192, 7, 7]               0\n",
      "          Conv2d-219           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-220           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-221           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-222           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-224           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-226             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-227             [-1, 48, 1, 1]               0\n",
      "          Conv2d-228           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-229           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-230           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-231            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-232            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-233            [-1, 192, 7, 7]               0\n",
      "          MBConv-234            [-1, 192, 7, 7]               0\n",
      "          Conv2d-235           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-236           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-237           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-238           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-240           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-242             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-243             [-1, 48, 1, 1]               0\n",
      "          Conv2d-244           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-245           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-246           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-247            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-248            [-1, 320, 7, 7]             640\n",
      "          MBConv-249            [-1, 320, 7, 7]               0\n",
      "          Conv2d-250           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-251           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-252           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0\n",
      "         Dropout-254                 [-1, 1280]               0\n",
      "          Linear-255                    [-1, 3]           3,843\n",
      "================================================================\n",
      "Total params: 4,011,391\n",
      "Trainable params: 3,843\n",
      "Non-trainable params: 4,007,548\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 173.64\n",
      "Params size (MB): 15.30\n",
      "Estimated Total Size (MB): 189.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad=True\n",
    "\n",
    "model.classifier=nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=3, bias=True)\n",
    ")\n",
    "model.to(device)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c468f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "def training(dataLoader, model, loss_fn, optimizer):\n",
    "    size=len(dataLoader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        X, y= X.to(device), y.to(device)\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss, current = loss.item(), (batch+1)*len(X)\n",
    "    print(f\"Loss: {loss}  [{current}/{size}]\")\n",
    "    \n",
    "def testing(dataLoader, model, loss_fn):\n",
    "    size=len(dataLoader.dataset)    \n",
    "    n_batches=len(dataLoader)\n",
    "    model.eval()\n",
    "    test_loss, correct= 0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred, y).item()\n",
    "\n",
    "            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss/=n_batches\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7663422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss: 1.3211660385131836  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.707175 \n",
      "\n",
      "Epoch 2:\n",
      "Loss: 1.4573718309402466  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.596720 \n",
      "\n",
      "Epoch 3:\n",
      "Loss: 1.1044596433639526  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.601943 \n",
      "\n",
      "Epoch 4:\n",
      "Loss: 1.3412692546844482  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.535182 \n",
      "\n",
      "Epoch 5:\n",
      "Loss: 1.086911678314209  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.548714 \n",
      "\n",
      "Epoch 6:\n",
      "Loss: 0.974144458770752  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.490777 \n",
      "\n",
      "Epoch 7:\n",
      "Loss: 1.6584638357162476  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.461498 \n",
      "\n",
      "Epoch 8:\n",
      "Loss: 1.0076453685760498  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.459437 \n",
      "\n",
      "Epoch 9:\n",
      "Loss: 1.379204511642456  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.533686 \n",
      "\n",
      "Epoch 10:\n",
      "Loss: 0.9481418132781982  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.471392 \n",
      "\n",
      "Epoch 11:\n",
      "Loss: 0.6770046353340149  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.441361 \n",
      "\n",
      "Epoch 12:\n",
      "Loss: 0.5976951122283936  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.424377 \n",
      "\n",
      "Epoch 13:\n",
      "Loss: 1.6609199047088623  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.409284 \n",
      "\n",
      "Epoch 14:\n",
      "Loss: 1.268099308013916  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.418249 \n",
      "\n",
      "Epoch 15:\n",
      "Loss: 1.071347951889038  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.474281 \n",
      "\n",
      "Epoch 16:\n",
      "Loss: 1.3248709440231323  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.468169 \n",
      "\n",
      "Epoch 17:\n",
      "Loss: 1.5295522212982178  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.452675 \n",
      "\n",
      "Epoch 18:\n",
      "Loss: 0.9230847358703613  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.370204 \n",
      "\n",
      "Epoch 19:\n",
      "Loss: 1.3025429248809814  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.363969 \n",
      "\n",
      "Epoch 20:\n",
      "Loss: 1.3496105670928955  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.386883 \n",
      "\n",
      "Epoch 21:\n",
      "Loss: 1.0733460187911987  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.377649 \n",
      "\n",
      "Epoch 22:\n",
      "Loss: 1.486426830291748  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.354488 \n",
      "\n",
      "Epoch 23:\n",
      "Loss: 1.06313955783844  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.368958 \n",
      "\n",
      "Epoch 24:\n",
      "Loss: 1.127885341644287  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.360587 \n",
      "\n",
      "Epoch 25:\n",
      "Loss: 1.5410383939743042  [8/225]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.391823 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=25\n",
    "for i in range(epoch):\n",
    "    print(f\"Epoch {i+1}:\")\n",
    "    training(train_dataloader, model, loss_fn, optimizer)\n",
    "    testing(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09ba358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 12, Predicted: \"sushi\", Actual: \"pizza\"\n",
      "at 13, Predicted: \"steak\", Actual: \"pizza\"\n",
      "at 16, Predicted: \"sushi\", Actual: \"pizza\"\n",
      "at 29, Predicted: \"pizza\", Actual: \"steak\"\n",
      "at 43, Predicted: \"sushi\", Actual: \"steak\"\n",
      "at 45, Predicted: \"pizza\", Actual: \"sushi\"\n",
      "at 46, Predicted: \"pizza\", Actual: \"sushi\"\n",
      "at 64, Predicted: \"steak\", Actual: \"sushi\"\n",
      "at 70, Predicted: \"steak\", Actual: \"sushi\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_data)):\n",
    "        X, y = test_data[i][0], test_data[i][1]\n",
    "        X = X.unsqueeze(0)\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        predicted, actual = class_names[pred[0].argmax(0)], class_names[y]\n",
    "        if predicted != actual:\n",
    "            print(f'at {i}, Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3eed6",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6c893cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General         Dresses    Dresses  \n",
       "1                        0  General Petite         Bottoms      Pants  \n",
       "2                        6         General            Tops    Blouses  \n",
       "3                        4         General         Dresses    Dresses  \n",
       "4                        1  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RNN \n",
    "\n",
    "df = pd.read_csv(r\"D:\\\\AppStoneLab\\\\TensorFlow\\Datasets\\\\Clothing-Review.csv\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "87d7d638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    0.770369\n",
       "0    0.229631\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_score(rating):\n",
    "    return int(rating > 3)\n",
    "\n",
    "df[\"Label\"] = df[\"Rating\"].apply(filter_score)\n",
    "\n",
    "df[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8188631d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dresses</td>\n",
       "      <td>major design flaw</td>\n",
       "      <td>high hope dress really wanted work initially o...</td>\n",
       "      <td>0</td>\n",
       "      <td>major design flaw high hope dress really wante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pants</td>\n",
       "      <td>favorite buy</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>1</td>\n",
       "      <td>favorite buy love love love jumpsuit fun flirt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blouses</td>\n",
       "      <td>flattering shirt</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "      <td>1</td>\n",
       "      <td>flattering shirt shirt flattering due adjustab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dresses</td>\n",
       "      <td>petite</td>\n",
       "      <td>love tracy reese dress one petite 5 foot tall ...</td>\n",
       "      <td>0</td>\n",
       "      <td>petite love tracy reese dress one petite 5 foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knits</td>\n",
       "      <td>cagrcoal shimmer fun</td>\n",
       "      <td>aded basket hte last mintue see would look lik...</td>\n",
       "      <td>1</td>\n",
       "      <td>cagrcoal shimmer fun aded basket hte last mint...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class Name                 Title  \\\n",
       "0    Dresses     major design flaw   \n",
       "1      Pants          favorite buy   \n",
       "2    Blouses      flattering shirt   \n",
       "3    Dresses                petite   \n",
       "4      Knits  cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Label  \\\n",
       "0  high hope dress really wanted work initially o...      0   \n",
       "1  love love love jumpsuit fun flirty fabulous ev...      1   \n",
       "2  shirt flattering due adjustable front tie perf...      1   \n",
       "3  love tracy reese dress one petite 5 foot tall ...      0   \n",
       "4  aded basket hte last mintue see would look lik...      1   \n",
       "\n",
       "                                                Text  \n",
       "0  major design flaw high hope dress really wante...  \n",
       "1  favorite buy love love love jumpsuit fun flirt...  \n",
       "2  flattering shirt shirt flattering due adjustab...  \n",
       "3  petite love tracy reese dress one petite 5 foo...  \n",
       "4  cagrcoal shimmer fun aded basket hte last mint...  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def toLower(data):\n",
    "    if isinstance(data, float):\n",
    "        return \"<UNK>\"\n",
    "    return str(data).lower()\n",
    "\n",
    "def remove_punctuation_func(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = []\n",
    "    for w in text.split():\n",
    "        if w not in stop_words:\n",
    "            words.append(w)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def lemmatize_sentence(text):\n",
    "    return \" \".join([lemm.lemmatize(w) for w in text.split()])\n",
    "\n",
    "X = df[[\"Class Name\", \"Title\", \"Review Text\", \"Label\"]].copy()\n",
    "\n",
    "X[\"Title\"] = X[\"Title\"].apply(toLower).apply(remove_punctuation_func).apply(remove_stopwords).apply(lemmatize_sentence)\n",
    "X[\"Review Text\"] = X[\"Review Text\"].apply(toLower).apply(remove_punctuation_func).apply(remove_stopwords).apply(lemmatize_sentence)\n",
    "\n",
    "X[\"Text\"] = X[\"Title\"] + \" \" + X[\"Review Text\"] + \" \" + X[\"Class Name\"].astype(str)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "44e7776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10000\n",
      "PAD id: 0\n",
      "UNK id: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_VOCAB = 10000   # like TensorFlow tokenizer(num_words=10000)\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for text in X[\"Text\"]:\n",
    "    counter.update(text.split())\n",
    "\n",
    "vocab = [\"<PAD>\", \"<UNK>\"]\n",
    "\n",
    "most_common_words = counter.most_common(MAX_VOCAB - len(vocab))\n",
    "vocab += [word for word, _ in most_common_words]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word2index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "pad_id = word2index[\"<PAD>\"]\n",
    "unk_id = word2index[\"<UNK>\"]\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"PAD id:\", pad_id)\n",
    "print(\"UNK id:\", unk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_padded shape: (19662, 40)\n",
      "X_padded dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def pad_features(reviews, pad_id, seq_length=40):\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=np.int64)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        row = row[:seq_length]\n",
    "        features[i, :len(row)] = np.array(row, dtype=np.int64)\n",
    "\n",
    "    return features\n",
    "\n",
    "seq_length = 40   \n",
    "\n",
    "encoding = []\n",
    "for text in X[\"Text\"]:\n",
    "    enc = [word2index.get(word, unk_id) for word in text.split()]\n",
    "    encoding.append(enc)\n",
    "\n",
    "X[\"Encoding\"] = encoding\n",
    "\n",
    "# Create padded matrix (numpy array)\n",
    "X_padded = pad_features(X[\"Encoding\"], pad_id, seq_length)\n",
    "\n",
    "print(\"X_padded shape:\", X_padded.shape)\n",
    "print(\"X_padded dtype:\", X_padded.dtype)\n",
    "\n",
    "X[\"Padded Encoding\"] = list(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "abfe54d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 77)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.25, random_state=100, stratify=y)\n",
    "\n",
    "trainset = TensorDataset(torch.tensor(X_train, dtype=torch.long),torch.tensor(y_train, dtype=torch.float32))\n",
    "\n",
    "testset = TensorDataset(torch.tensor(X_test, dtype=torch.long),torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "621fbd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(10000, 128, padding_idx=0)\n",
      "  (rnn1): RNN(128, 64, batch_first=True)\n",
      "  (rnn2): RNN(64, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_id, embed_dim=128, hidden_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "\n",
    "        self.rnn1 = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.rnn2 = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)         \n",
    "\n",
    "        out1, _ = self.rnn1(x)         \n",
    "        out2, h2 = self.rnn2(out1)     \n",
    "\n",
    "        last_hidden = h2[-1]          \n",
    "\n",
    "        x = self.fc1(last_hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.fc2(x).squeeze(1)\n",
    "        return logits\n",
    "    \n",
    "model = RNN(vocab_size=vocab_size, pad_id=pad_id).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ada50321",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "# loss_fn=nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def f1_score_binary(y_true, y_pred):\n",
    "    # y_true, y_pred are tensors of 0/1 floats\n",
    "    tp = ((y_true == 1) & (y_pred == 1)).sum().item()\n",
    "    fp = ((y_true == 0) & (y_pred == 1)).sum().item()\n",
    "    fn = ((y_true == 1) & (y_pred == 0)).sum().item()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def training(dataLoader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in dataLoader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)  # logits\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(pred)\n",
    "        preds = (probs >= 0.5).float()\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def testing(dataLoader, model, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_true.append(y)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        acc = correct / total\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_true = torch.cat(all_true)\n",
    "\n",
    "        f1, precision, recall = f1_score_binary(all_true, all_preds)\n",
    "\n",
    "        # print(f\"Test -> loss: {avg_loss:.4f} | acc: {acc*100:.2f}% | \"f\"F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "11f2e7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train -> loss: 0.3168 | acc: 55.15%\n",
      "Test  -> loss: 0.2977 | acc: 64.44% | F1: 0.7282\n",
      "--------------------------------------------------\n",
      "Epoch 2/10\n",
      "Train -> loss: 0.2845 | acc: 70.66%\n",
      "Test  -> loss: 0.3271 | acc: 45.30% | F1: 0.4969\n",
      "--------------------------------------------------\n",
      "Epoch 3/10\n",
      "Train -> loss: 0.2737 | acc: 70.93%\n",
      "Test  -> loss: 0.2713 | acc: 78.97% | F1: 0.8614\n",
      "--------------------------------------------------\n",
      "Epoch 4/10\n",
      "Train -> loss: 0.2432 | acc: 78.62%\n",
      "Test  -> loss: 0.2521 | acc: 79.96% | F1: 0.8667\n",
      "--------------------------------------------------\n",
      "Epoch 5/10\n",
      "Train -> loss: 0.2561 | acc: 73.43%\n",
      "Test  -> loss: 0.2700 | acc: 73.41% | F1: 0.8129\n",
      "--------------------------------------------------\n",
      "Epoch 6/10\n",
      "Train -> loss: 0.2530 | acc: 78.73%\n",
      "Test  -> loss: 0.2631 | acc: 76.69% | F1: 0.8418\n",
      "--------------------------------------------------\n",
      "Epoch 7/10\n",
      "Train -> loss: 0.2492 | acc: 77.97%\n",
      "Test  -> loss: 0.2774 | acc: 74.00% | F1: 0.8194\n",
      "--------------------------------------------------\n",
      "Epoch 8/10\n",
      "Train -> loss: 0.2344 | acc: 78.90%\n",
      "Test  -> loss: 0.2556 | acc: 80.02% | F1: 0.8674\n",
      "--------------------------------------------------\n",
      "Epoch 9/10\n",
      "Train -> loss: 0.2182 | acc: 82.45%\n",
      "Test  -> loss: 0.2589 | acc: 76.97% | F1: 0.8429\n",
      "--------------------------------------------------\n",
      "Epoch 10/10\n",
      "Train -> loss: 0.2191 | acc: 78.61%\n",
      "Test  -> loss: 0.2597 | acc: 75.55% | F1: 0.8289\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = training(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss, test_acc, test_f1 = testing(test_dataloader, model, loss_fn)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train -> loss: {train_loss:.4f} | acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test  -> loss: {test_loss:.4f} | acc: {test_acc*100:.2f}% | F1: {test_f1:.4f}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56d0cd",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
